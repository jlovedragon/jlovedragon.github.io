---
layout: post
title: k近邻法
categories: [Machine Learning]
---

## 前言
k近邻法(k-nearest neighbor, k-NN)是一种基本的分类与回归的非参数统计方法。

```
k-NN分类：一个对象的分类是由其邻居的“多数表决”确定的，k 个最近邻居中最常见的分类决定了赋予该对象的类别。若 k = 1，则该对象的类别直接由最近的一个节点赋予。
k-NN回归：输出是其 k 个最近邻居的值的平均值。
```
k-NN是一种`基于实例`的学习，或者是局部近似和将所有计算推迟到分类之后的惰性学习。k-近邻算法是所有的机器学习算法中最简单的之一。
本文主要介绍k-NN的分类问题。

<!-- more -->

## k-NN简介
给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最临近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这类。

## k-NN模型三要素
```
距离度量: 一般采用欧氏距离。不同的距离度量所确定的最近邻点不同。
k值选择：k一般取比较小的数值。但k值过小，模型越复杂，容易过拟合。
分类决策规则：往往采用多数表决。
```

## k-NN实现：kd树
实现k-NN主要考虑的问题是如果对训练数据进行快速k近邻搜索。最简单的实现方法是线性扫描，但当训练集很大时，效率很低。为了提高k近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，已减少计算距离的次数，下面要介绍的就是kd树方法。

### 构造kd树
kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的二叉树结构。构造kd数相当于不断地用垂直于坐标轴的超平面将k维空间划分，构成一系列的k维超矩形区域。
方法如下：

1. 构造根节点，对应于包含所有实例点的k维空间的超矩形区域。
2. 选择以$x^{(1)}$为坐标轴，以所有实例的$x^{(1)}$坐标的中位数作为切分点，用通过切分点并与坐标轴垂直的超平面将超矩形区域划分为两个子区域，左子结点对应与坐标$x^{(1)}$小于切分点的子区域，右子节点对应与坐标$x^{(1)}$大于切分点的子区域。
3. 重复：对深度为j的节点，选择$x^{(l)}$为切分的坐标轴，$l=(j mode k) + 1$。将落在切分超平面上的实例点保存在该节点。
4. 直到两个子区域没有实例存在时停止。从而形成kd树的区域划分。

### 搜索kd树
给定一个目标点，搜索其最近邻点，首先先找到包含目标点的叶结点，然后从该结点出发，依次回退到父结点；不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。

1. 在kd树上找到包含目标点的叶结点，并以此叶结点作为“当前最近点”。
2. 递归地向上回退，在每个结点进行以下操作：

    2.1 如果该节点保存的实例点比当前最近点距离目标点更近，则已该实例点为“当前最近点”。

    2.2 当前最近点一定存在于该结点一个子节点对应的区域。检查该子结点的父结点的另一子节点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为中心，以目标点与“当前实例点”间的距离为半径的超球体相交。如果相交，则可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一子结点，接着递归地进行最近邻搜索，如果不相交，向上回退。

3. 当回退到根结点时，搜索结束，最后的“当前最近点”即为最近邻点。

### 总结
如果实例点随机分布，则kd树的搜索平均计算复杂度为O(logN)，N为训练实例。kd树更适合于训练实例数远大于空间维度数时的k-NN。而空间维度数接近训练实例时，几乎接近线性扫描。

## 参考资料
1. [《统计学习方法》](https://book.douban.com/subject/10590856/)
2. [维基百科](https://zh.wikipedia.org/wiki/%E6%9C%80%E8%BF%91%E9%84%B0%E5%B1%85%E6%B3%95)
