<p>在落网上听音乐是一个非常享受的事情，唯美的封面、文字，瞬间就觉得自己置身于一个没有喧嚣、没有嘈杂的环境里，一切尽在不言中～
最近在看爬虫相关的资料，想想正好用python爬爬落网练练手，说干就干，下面大致说下我爬落网的过程:</p>

<h2 id="section">明确目标</h2>

<ol>
  <li>爬下落网所有期的音乐，并按每期的名字建立目录，将该期的音乐存在所属期的目录下</li>
  <li>程序能自动下载当前的最新一期</li>
  <li>每首音乐需正确命名，因为落网的音乐链接使用编号命名的，得改用正确的歌名命名</li>
</ol>

<p>先来两张图看看效果：</p>

<ul>
  <li>
    <p>目录：
<img src="http://ww4.sinaimg.cn/large/6120fe13jw1entwctqx52j20wh06pdkg.jpg" alt="" /></p>
  </li>
  <li>
    <p>文件：
<img src="http://ww3.sinaimg.cn/large/6120fe13jw1entwf5kdp1j20kb070jv3.jpg" alt="" /></p>
  </li>
</ul>

<h2 id="section-1">前期准备</h2>
<p>先说下我所用的工具：Archlinux + Emacs24.4 + python2.7 + firefox</p>

<p>Q：该用什么第三方库？
&gt; python关于爬虫的第三方库何其多，这里我提供两套方案：
&gt;
&gt; 1. <a href="http://requests.readthedocs.org/zh_CN/latest/">Requests</a> 加载网页 + <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html">BeautifulSoup</a> 解析网页
&gt; 2. <a href="http://scrapy.org">Scrapy</a>，<code>xpath</code>很好用</p>

<h2 id="section-2">抓取思路</h2>
<p>两套方案的思路都是一样的：首先我得知道最新一期的编号，如果比我已经下载的期数大，则从此期数开始遍历一直到已经爬过的期数，进入每期单独的页面，得到每期的名字用于建立目录，每期的歌单有多少歌以及歌名用于下载和重命名。代码这块我就只讲<code>Requests</code>+<code>BeautifulSoup</code>:</p>

<ol>
  <li>
    <p>获取最新一期的编号，用Firebug查看html：
<img src="http://ww1.sinaimg.cn/large/6120fe13jw1entxgq1gcnj20fc03cq3o.jpg" alt="" /></p>

    <p>```python
 numberUrl = ‘http://www.luoo.net’
 def getTheMaxNumber():
     # get the html
     numberResponse = requests.get(numberUrl)
     numberResponse.encoding = ‘utf-8’
     numberSoup = BeautifulSoup(numberResponse.text, ‘lxml’)
     # parse the html and get the number
     numberText = (numberSoup.find_all(‘div’, ‘vol-item-lg’)[0]).text</p>

    <pre><code> maxNumber = int(re.findall(r'\d+', numberText)[0])
 return maxNumber  ```
</code></pre>
  </li>
  <li>
    <p>进行比较，先读取保存文件目录下的number.txt看上次下载到了哪一期，如果最新期数大，则进入下载音乐的函数，否则返回：</p>

    <p><code>python
 # compare
 if maxNumber &gt; currentNumber:
     for i in range(currentNumber + 1, maxNumber + 1):
         downloadMusic(i)
     # after download the music, should write the right number to number.txt
     numberFile.write(str(maxNumber) + '\t' + time.strftime('%Y-%m-%d %H:%M:%S') + '\n')
     numberFile.close()
 else:
     # do nothing
     print '您已经下载了所有的期数，谢谢！\n'
     return
</code></p>
  </li>
  <li>
    <p>建立目录并下载音乐：
先用Firebug查看页面：</p>
  </li>
</ol>

<p><img src="http://ww4.sinaimg.cn/large/6120fe13jw1entxwx6hi8j20ao01vweq.jpg" alt="" /></p>

<p><img src="http://ww2.sinaimg.cn/large/6120fe13jw1entxzc87ehj20gn01imxk.jpg" alt="" /></p>

<ul>
  <li>
    <p>得到期数ID</p>

    <p><code>python
  albumId = (albumSoup.find_all('span', 'vol-number rounded')[0]).text
 </code></p>
  </li>
  <li>
    <p>得到期数名字用来建立目录，最好是将名字间的空格去掉，不然后面wget时会出现问题</p>

    <p><code>python
  albumName = (albumSoup.find_all('span', 'vol-title')[0]).text.replace(' ', '_')
 </code></p>
  </li>
  <li>
    <p>得到每首歌的名字用来重命名音乐</p>

    <p><code>python
  musicList = (albumSoup.find_all('a', 'trackname btn-play'))
 </code></p>
  </li>
  <li>
    <p>建立目录：</p>

    <p><code>python
  isExists = os.path.exists(saveDir)
  if not isExists:
      os.makedirs(saveDir)
 </code></p>
  </li>
  <li>
    <p>使用wget下载mp3:</p>

    <p>```python
  for i in range(1, len(musicList) + 1):
          currentMusicName = musicList[i - 1].text.replace(‘ ‘, ‘<em>’).replace(‘.</em>’, ‘.’).strip(‘(‘).strip(‘)’)
          cmd1 = ‘wget ‘ + musicUrl + str(int(albumId)) + ‘/’ + string.zfill(i,2) + ‘.mp3 ‘ + ‘-O ‘ + saveDir + currentMusicName + ‘.mp3’
          cmd2 = ‘wget ‘ + musicUrl + str(int(albumId)) + ‘/’ + str(i) + ‘.mp3 ‘ + ‘-O ‘ + saveDir + currentMusicName + ‘.mp3’</p>

    <pre><code>      if os.system(cmd1.encode('utf-8')) != 0:
          os.system(cmd2.encode('utf-8'))   ```
</code></pre>
  </li>
</ul>

<h2 id="section-3">完整代码</h2>
<p>完整代码请移步我的<a href="https://github.com/jlovedragon/CrawLuoo/tree/master/requests_bs4">github</a>
另一种方案Scrapy的思路一样，<a href="https://github.com/jlovedragon/CrawLuoo/tree/master/scrapy">代码</a></p>

<h2 id="section-4">参考文献</h2>
<ol>
  <li><a href="http://requests.readthedocs.org/zh_CN/latest/">Requests: HTTP for Humans</a></li>
  <li><a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html">Beautiful Soup 4.2.0 文档</a></li>
  <li><a href="http://chenqx.github.io/2014/11/09/Scrapy-Tutorial-for-BBSSpider/">Scrapy爬虫抓取网站数据</a></li>
</ol>

