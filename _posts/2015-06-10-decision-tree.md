---
layout: post
title: 决策树
categories: [Machine Learning]
---

## 简介
分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点和有向边组成，借点有两种类型：内部结点和叶结点。

## 特征选择
选取对训练数据具有分类能力的特征，提高决策树的学习效率。通常采用`信息增益(ID3)`或`信息增益比(C4.5)`为特征选择的准则。

### 熵(entropy)
熵为随机变量不确定的度量，定义为信息的期望值，名字来源于信息论之父香农。

设随机变量X是一个取有限个值的离散随机变量，其概率分布为: $$ P(X=x_i) = p_i, i = 1,2,...,n $$

则随机变量X的熵为: $$ H(X) = -\sum_{i=1}^n p_i log(p_i) $$

若对数以2为底，则熵的单位为比特(bit)，从信息论中熵的一种解释是熵确定了要编码的集合中任意成员的分类所需要的最少二进制位数。
熵越大，则随机变量的不确定性越大。

### 条件熵
设有随机变量(X,Y)，其联合概率分布为:
$$ H(X=x\_i, Y=y\_j) = p\_{ij}, i=1,2,...,n,j=1,2,...,m $$

条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望：
$$ H(Y|X) = \sum_{i=1}^n p\_i H(Y|X=x\_i), 其中p\_i=P(X=x\_i) $$

### **信息增益**
信息增益为得知特征X的信息而使得类Y的信息不确定性减少的程度，定义为特征A对训练集D的信息增益为g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：
$$ g(D,A) = H(D) - H(D|A) $$

一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。
信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征往往具有更强的分类能力。

#### 求信息增益的例子
|Day|Outlook|Temperature|Humidity|Wind|PlayTennis|
|:-:|:-:|:-:|:-:|:-:|:-:|
|D1|Sunny|Hot|High|Weak|No|
|D2|Sunny|Hot|High|Strong|No|
|D3|Overcast|Hot|High|Weak|Yes|
|D4|Rain|Mild|High|Weak|Yes|
|D5|Rain|Cool|Normal|Weak|Yes|
|D6|Rain|Cool|Normal|Strong|No|
|D7|Overcast|Cool|Normal|Strong|Yes|
|D8|Sunny|Mild|High|Weak|No|
|D9|Sunny|Cool|Normal|Weak|Yes|
|D10|Rain|Mild|Normal|Weak|Yes|
|D11|Sunny|Mild|Normal|Strong|Yes|
|D12|Overcast|Mild|High|Strong|Yes|
|D13|Overcast|Hot|Normal|Weak|Yes|
|D14|Rain|Mild|High|Strong|No|

训练集D相对于PlayTennis的熵为：

可以看到，Gain(D,Outlook)信息增益最大，故选择Outlook作为根结点，并为它的每一个可能的值在根节点下创建分支。接着递归地算子节点的信息增益，选择信息增益值最大的作为结点，直到将所有的类分开为止。

信息增益比较偏袒具有较多值的属性，因为太多的可能值必然会把训练集分割成非常小的空间，因此，相比训练集，它会有非常高的信息增益，尽管实际情况并非这样。

避免这个不足的方法是用信息增益比作为特征选择的度量准则。

### **信息增益比**
信息增益比通过假如一个被称作`分裂信息`的项来惩罚类似Date的属性，分裂信息用来衡量属性分裂数据的广度和均匀性：
## ID3算法
核心：在决策树各个结点上应用`信息增益`准则选择特征。
具体方法是：从根节点出发，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点，再对子结点递归地调用以上方法，构建决策树，直到所有特征的信息增益均很小或没有特征可以选择为止，最后得到一个决策树。
ID3算法存在的`缺点`：

1. ID3算法在选择根节点和内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，但在有些情况下这类属性可能不会提供太多有价值的信息。
2. ID3算法只能对描述属性为离散型属性的数据集构造决策树。

## C4.5算法
C4.5算法之所以是最常用的决策树算法，是因为它继承了ID3算法的所有优点并对ID3算法进行了改进和补充。C4.5算法采用信息增益率作为选择分支属性的标准，并克服了ID3算法中信息增益选择属性时偏向选择取值多的属性的不足，并能够完成对连续属性离散化的处理；还能够对不完整数据进行处理。C4.5算法属于基于信息论Information Theory的方法，以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳和分类。
C4.5算法主要做出了以下方面的改进:

1. 可以处理连续数值型属性
2. 用`信息增益比`来选择属性
3. 后剪枝策略
4. 缺失值处理

C4.5的`缺点`：

1. 算法低效，在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效
2. 内存受限，适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。

## 参考资料：
1. [《统计学习方法》](https://book.douban.com/subject/10590856/)
2. [《机器学习》](http://book.douban.com/subject/1102235/)
